{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://testpolly.elucidata.io/manage/workspaces?action=open_polly_notebook&source=github&path=path_place_holder&kernel=elucidata/Python 3.10&machine=medium\" target=\"_parent\"><img src=\"https://elucidatainc.github.io/PublicAssets/open_polly.svg\" alt=\"Open in Polly\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Welcome to Polly Python3 Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
            "Collecting polly-python==0.2.8-dataset-validation\n",
            "  Downloading https://elucidatainc.github.io/PublicAssets/builds/polly-python/tests/testpolly/polly_python-0.2.8_dataset_validation-py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 19.1 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting polly-validator@ https://elucidatainc.github.io/PublicAssets/builds/polly_validator-0.0.1-py3-none-any.whl\n",
            "  Downloading https://elucidatainc.github.io/PublicAssets/builds/polly_validator-0.0.1-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/site-packages (from polly-python==0.2.8-dataset-validation) (2.10)\n",
            "Collecting retrying==1.3.3\n",
            "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
            "Collecting pydantic\n",
            "  Downloading pydantic-1.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 26.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting cryptography>=38.0.1\n",
            "  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 98.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting mixpanel>=4.10.0\n",
            "  Downloading mixpanel-4.10.0-py2.py3-none-any.whl (8.9 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/site-packages (from polly-python==0.2.8-dataset-validation) (2.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from polly-python==0.2.8-dataset-validation) (1.4.1)\n",
            "Collecting joblib\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 134.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting sqlparse\n",
            "  Downloading sqlparse-0.4.3-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 3.7 MB/s s eta 0:00:01\n",
            "\u001b[?25hCollecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 139.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting python-jose>=3.3.0\n",
            "  Downloading python_jose-3.3.0-py2.py3-none-any.whl (33 kB)\n",
            "Collecting Cerberus==1.3.2\n",
            "  Downloading Cerberus-1.3.2.tar.gz (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 3.1 MB/s s eta 0:00:01\n",
            "\u001b[?25hCollecting pytest\n",
            "  Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 107.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting certifi==2021.10.8\n",
            "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 136.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: boto3>=1.17.73 in /usr/local/lib/python3.10/site-packages (from polly-python==0.2.8-dataset-validation) (1.21.19)\n",
            "Collecting python-magic==0.4.24\n",
            "  Downloading python_magic-0.4.24-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/site-packages (from polly-python==0.2.8-dataset-validation) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from polly-python==0.2.8-dataset-validation) (4.64.1)\n",
            "Collecting rst2txt\n",
            "  Downloading rst2txt-1.1.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: urllib3==1.26.6 in /usr/local/lib/python3.10/site-packages (from polly-python==0.2.8-dataset-validation) (1.26.6)\n",
            "Collecting datapane\n",
            "  Downloading datapane-0.15.5-py3-none-any.whl (7.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2 MB 127.5 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/site-packages (from polly-python==0.2.8-dataset-validation) (4.0.0)\n",
            "Collecting pytz==2021.1\n",
            "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 111.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting requests==2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 18.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting elucidatacmapPy==3.3.4\n",
            "  Downloading elucidatacmapPy-3.3.4-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 110.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting postpy2==0.0.6\n",
            "  Downloading postpy2-0.0.6-py3-none-any.whl (17 kB)\n",
            "Collecting Deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting elucidatacloudpathlib==0.6.6\n",
            "  Downloading elucidatacloudpathlib-0.6.6-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 15.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting plotly\n",
            "  Downloading plotly-5.11.0-py2.py3-none-any.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 101.4 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: botocore>=1.20.73 in /usr/local/lib/python3.10/site-packages (from polly-python==0.2.8-dataset-validation) (1.24.19)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from Cerberus==1.3.2->polly-python==0.2.8-dataset-validation) (58.1.0)\n",
            "Collecting h5py>=2.6.0\n",
            "  Downloading h5py-3.7.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 119.8 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.10/site-packages (from elucidatacmapPy==3.3.4->polly-python==0.2.8-dataset-validation) (1.22.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.10/site-packages (from boto3>=1.17.73->polly-python==0.2.8-dataset-validation) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.10/site-packages (from boto3>=1.17.73->polly-python==0.2.8-dataset-validation) (0.5.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/site-packages (from cryptography>=38.0.1->polly-python==0.2.8-dataset-validation) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=38.0.1->polly-python==0.2.8-dataset-validation) (2.21)\n",
            "Requirement already satisfied: rsa in /usr/local/lib/python3.10/site-packages (from python-jose>=3.3.0->polly-python==0.2.8-dataset-validation) (4.7.2)\n",
            "Collecting ecdsa!=0.15\n",
            "  Downloading ecdsa-0.18.0-py2.py3-none-any.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 132.1 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: pyasn1 in /usr/local/lib/python3.10/site-packages (from python-jose>=3.3.0->polly-python==0.2.8-dataset-validation) (0.4.8)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: nbconvert<7.0.0,>=6.1.0 in /usr/local/lib/python3.10/site-packages (from datapane->polly-python==0.2.8-dataset-validation) (6.3.0)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.4.0 in /usr/local/lib/python3.10/site-packages (from datapane->polly-python==0.2.8-dataset-validation) (5.4.1)\n",
            "Collecting toolz<0.13.0,>=0.11.0\n",
            "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 9.7 MB/s s eta 0:00:01\n",
            "\u001b[?25hCollecting validators<0.21.0,>=0.18.0\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "Collecting requests-toolbelt<0.10.0,>=0.9.1\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 6.6 MB/s s eta 0:00:01\n",
            "\u001b[?25hCollecting importlib_resources<6.0.0,>=3.0.0\n",
            "  Downloading importlib_resources-5.10.1-py3-none-any.whl (34 kB)\n",
            "Collecting boltons<22.0.0,>=20.0.0\n",
            "  Downloading boltons-21.0.0-py2.py3-none-any.whl (193 kB)\n",
            "\u001b[K     |████████████████████████████████| 193 kB 130.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting click-spinner<0.2.0,>=0.1.8\n",
            "  Downloading click_spinner-0.1.10-py2.py3-none-any.whl (4.0 kB)\n",
            "Collecting posthog<3.0.0,>=1.4.0\n",
            "  Downloading posthog-2.2.0-py2.py3-none-any.whl (33 kB)\n",
            "Collecting ipynbname<2022.0.0,>=2021.3.2\n",
            "  Downloading ipynbname-2021.3.2-py3-none-any.whl (4.0 kB)\n",
            "Collecting furl<3.0.0,>=2.0.0\n",
            "  Downloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting datacommons-pandas<0.0.4,>=0.0.3\n",
            "  Downloading datacommons_pandas-0.0.3-py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 10.2 MB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting datacommons<2.0.0,>=1.4.3\n",
            "  Downloading datacommons-1.4.3-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 9.5 MB/s s eta 0:00:01\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25hCollecting colorlog<7.0.0,>=4.1.0\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting micawber>=0.5.0\n",
            "  Downloading micawber-0.5.4.tar.gz (18 kB)\n",
            "Collecting dacite<2.0.0,>=1.0.2\n",
            "  Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting glom<23.0.0,>=20.11.0\n",
            "  Downloading glom-22.1.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 28.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting lxml<5.0.0,>=4.0.0\n",
            "  Downloading lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 84.1 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting vega-datasets<1.0.0,>=0.9.0\n",
            "  Downloading vega_datasets-0.9.0-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 134.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting stringcase<2.0.0,>=1.2.0\n",
            "  Downloading stringcase-1.2.0.tar.gz (3.0 kB)\n",
            "Collecting dulwich<0.21.0,>=0.20.0\n",
            "  Downloading dulwich-0.20.50-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (499 kB)\n",
            "\u001b[K     |████████████████████████████████| 499 kB 123.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting dominate<3.0.0,>=2.4.0\n",
            "  Downloading dominate-2.7.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting pyarrow<11.0.0,>=6.0.0\n",
            "  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 35.9 MB 113.4 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: Jinja2<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/site-packages (from datapane->polly-python==0.2.8-dataset-validation) (3.0.3)\n",
            "Collecting packaging<22.0.0,>=20.0.0\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 12.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting click<9.0.0,>=7.1.0\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 16.1 MB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting altair<5.0.0,>=4.0.0\n",
            "  Downloading altair-4.2.0-py3-none-any.whl (812 kB)\n",
            "\u001b[K     |████████████████████████████████| 812 kB 126.6 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: jsonschema<5.0.0,>=3.2.0 in /usr/local/lib/python3.10/site-packages (from datapane->polly-python==0.2.8-dataset-validation) (4.2.1)\n",
            "Collecting munch<3.0.0,>=2.3.0\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting tabulate<0.9.0,>=0.8.0\n",
            "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/site-packages (from altair<5.0.0,>=4.0.0->datapane->polly-python==0.2.8-dataset-validation) (0.3)\n",
            "Collecting orderedmultidict>=1.0.1\n",
            "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/site-packages (from glom<23.0.0,>=20.11.0->datapane->polly-python==0.2.8-dataset-validation) (21.2.0)\n",
            "Collecting face>=20.1.0\n",
            "  Downloading face-22.0.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 8.7 MB/s s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.10/site-packages (from ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (6.13.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from Jinja2<4.0.0,>=3.0.0->datapane->polly-python==0.2.8-dataset-validation) (2.0.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/site-packages (from jsonschema<5.0.0,>=3.2.0->datapane->polly-python==0.2.8-dataset-validation) (0.18.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (1.5.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (4.9.1)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.10/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (5.1.3)\n",
            "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.10/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (0.5.13)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.10/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (0.5.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (0.2.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (0.7.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (2.10.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (4.1.0)\n",
            "Requirement already satisfied: traitlets>=5.0 in /usr/local/lib/python3.10/site-packages (from nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client>=6.1.5 in /usr/local/lib/python3.10/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (6.1.12)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (1.5.6)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (22.3.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/site-packages (from nbformat>=4.4->nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (0.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/site-packages (from packaging<22.0.0,>=20.0.0->datapane->polly-python==0.2.8-dataset-validation) (3.0.6)\n",
            "Collecting backoff<2.0.0,>=1.10.0\n",
            "  Downloading backoff-1.11.1-py2.py3-none-any.whl (13 kB)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting typing-extensions>=4.1.0\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.10/site-packages (from validators<0.21.0,>=0.18.0->datapane->polly-python==0.2.8-dataset-validation) (5.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/site-packages (from bleach->nbconvert<7.0.0,>=6.1.0->datapane->polly-python==0.2.8-dataset-validation) (0.5.1)\n",
            "Collecting wrapt<2,>=1.10\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 15.7 MB/s  eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.10/site-packages (from ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (1.6.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (5.9.4)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/site-packages (from ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (0.1.3)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/site-packages (from ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (8.2.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (3.0.22)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (0.7.5)\n",
            "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (0.6.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (4.8.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (0.18.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (0.2.5)\n",
            "Collecting tenacity>=6.2.0\n",
            "  Downloading tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
            "Collecting pronto\n",
            "  Downloading pronto-2.5.2-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 17.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting rapidfuzz==2.1.1\n",
            "  Downloading rapidfuzz-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 129.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting appdirs==1.4.4\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting jarowinkler<1.1.0,>=1.0.3\n",
            "  Downloading jarowinkler-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 120.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting fastobo~=0.12.2\n",
            "  Downloading fastobo-0.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 106.7 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: networkx~=2.3 in /usr/local/lib/python3.10/site-packages (from pronto->polly-validator@ https://elucidatainc.github.io/PublicAssets/builds/polly_validator-0.0.1-py3-none-any.whl->polly-python==0.2.8-dataset-validation) (2.8.8)\n",
            "Collecting pronto\n",
            "  Downloading pronto-2.5.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 19.5 MB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting tomli>=1.0.0\n",
            "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.10/site-packages (from rst2txt->polly-python==0.2.8-dataset-validation) (0.15.2)\n",
            "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (0.2.2)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (2.2.1)\n",
            "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane->polly-python==0.2.8-dataset-validation) (1.2.0)\n",
            "Using legacy 'setup.py install' for Cerberus, since package 'wheel' is not installed.\n",
            "Using legacy 'setup.py install' for retrying, since package 'wheel' is not installed.\n",
            "Using legacy 'setup.py install' for micawber, since package 'wheel' is not installed.\n",
            "Using legacy 'setup.py install' for stringcase, since package 'wheel' is not installed.\n",
            "Using legacy 'setup.py install' for validators, since package 'wheel' is not installed.\n",
            "Installing collected packages: pytz, packaging, certifi, boltons, typing-extensions, toolz, requests, orderedmultidict, monotonic, jarowinkler, fastobo, face, backoff, wrapt, vega-datasets, validators, tomli, tenacity, tabulate, stringcase, soupsieve, requests-toolbelt, rapidfuzz, python-magic, pydantic, pyarrow, pronto, posthog, pluggy, munch, micawber, lxml, ipynbname, iniconfig, importlib-resources, h5py, glom, furl, exceptiongroup, ecdsa, dulwich, dominate, datacommons-pandas, datacommons, dacite, colorlog, click-spinner, click, appdirs, altair, sqlparse, rst2txt, retrying, python-jose, pytest, postpy2, polly-validator, plotly, mixpanel, joblib, elucidatacmapPy, elucidatacloudpathlib, Deprecated, datapane, cryptography, Cerberus, beautifulsoup4, polly-python\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2021.3\n",
            "    Uninstalling pytz-2021.3:\n",
            "      Successfully uninstalled pytz-2021.3\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 22.0\n",
            "    Uninstalling packaging-22.0:\n",
            "      Successfully uninstalled packaging-22.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.5.30\n",
            "    Uninstalling certifi-2021.5.30:\n",
            "      Successfully uninstalled certifi-2021.5.30\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "    Running setup.py install for validators ... \u001b[?25ldone\n",
            "\u001b[?25h  Attempting uninstall: tabulate\n",
            "    Found existing installation: tabulate 0.9.0\n",
            "    Uninstalling tabulate-0.9.0:\n",
            "      Successfully uninstalled tabulate-0.9.0\n",
            "    Running setup.py install for stringcase ... \u001b[?25ldone\n",
            "\u001b[?25h    Running setup.py install for micawber ... \u001b[?25ldone\n",
            "\u001b[?25h    Running setup.py install for retrying ... \u001b[?25ldone\n",
            "\u001b[?25h  Attempting uninstall: Cerberus\n",
            "    Found existing installation: Cerberus 1.3.4\n",
            "    Uninstalling Cerberus-1.3.4:\n",
            "      Successfully uninstalled Cerberus-1.3.4\n",
            "    Running setup.py install for Cerberus ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed Cerberus-1.3.2 Deprecated-1.2.13 altair-4.2.0 appdirs-1.4.4 backoff-1.11.1 beautifulsoup4-4.11.1 boltons-21.0.0 certifi-2021.10.8 click-8.1.3 click-spinner-0.1.10 colorlog-6.7.0 cryptography-38.0.4 dacite-1.6.0 datacommons-1.4.3 datacommons-pandas-0.0.3 datapane-0.15.5 dominate-2.7.0 dulwich-0.20.50 ecdsa-0.18.0 elucidatacloudpathlib-0.6.6 elucidatacmapPy-3.3.4 exceptiongroup-1.0.4 face-22.0.0 fastobo-0.12.2 furl-2.1.3 glom-22.1.0 h5py-3.7.0 importlib-resources-5.10.1 iniconfig-1.1.1 ipynbname-2021.3.2 jarowinkler-1.0.5 joblib-1.2.0 lxml-4.9.2 micawber-0.5.4 mixpanel-4.10.0 monotonic-1.6 munch-2.5.0 orderedmultidict-1.0.1 packaging-21.3 plotly-5.11.0 pluggy-1.0.0 polly-python-0.2.8 polly-validator-0.0.1 posthog-2.2.0 postpy2-0.0.6 pronto-2.5.1 pyarrow-10.0.1 pydantic-1.10.2 pytest-7.2.0 python-jose-3.3.0 python-magic-0.4.24 pytz-2021.1 rapidfuzz-2.1.1 requests-2.25.1 requests-toolbelt-0.9.1 retrying-1.3.3 rst2txt-1.1.0 soupsieve-2.3.2.post1 sqlparse-0.4.3 stringcase-1.2.0 tabulate-0.8.10 tenacity-8.1.0 tomli-2.0.1 toolz-0.12.0 typing-extensions-4.4.0 validators-0.20.0 vega-datasets-0.9.0 wrapt-1.14.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\r\n",
            "You should consider upgrading via the '/usr/local/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\r\n"
          ]
        }
      ],
      "source": [
        "!sudo pip3 install https://elucidatainc.github.io/PublicAssets/builds/polly-python/tests/testpolly/polly_python-0.2.8_dataset_validation-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from polly.auth import Polly\n",
        "AUTH_TOKEN=(os.environ['POLLY_REFRESH_TOKEN']) # Obtain authentication tokens\n",
        "Polly.auth(AUTH_TOKEN)\n",
        "from polly.omixatlas import OmixAtlas\n",
        "omixatlas = OmixAtlas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
            "\u001b7\u001b[?7l\u001b[1Gprogress [░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0% | NA | ETA: 0s | time elapsed: 0s\u001b[0K\u001b[1Gprogress [██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25% | 2.184 KB/8.633 KB | ETA: 1s | time elapsed: 0s\u001b[0K\u001b[1Gprogress [████████████████████████████████████████] 100% | 8.633 KB/8.633 KB | ETA: 0s | time elapsed: 0s\u001b[0K\u001b[?7h\u001b8\n",
            "\u001b[32m\u001b[1mSuccess: Sync complete\u001b[22m\u001b[39m\n"
          ]
        }
      ],
      "source": [
        "!polly files sync -s \"polly://metadata/\" -d \"metadata/\" -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
            "\u001b7\u001b[?7l\u001b[1Gprogress [░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0% | NA | ETA: 0s | time elapsed: 0s\u001b[0K\u001b[1Gprogress [████████████████████░░░░░░░░░░░░░░░░░░░░] 50% | 66.805 KB/133.591 KB | ETA: 1s | time elapsed: 0s\u001b[0K\u001b[1Gprogress [████████████████████████████████████████] 100% | 133.591 KB/133.591 KB | ETA: 0s | time elapsed: 0s\u001b[0K\u001b[?7h\u001b8\n",
            "\u001b[32m\u001b[1mSuccess: Sync complete\u001b[22m\u001b[39m\n"
          ]
        }
      ],
      "source": [
        "!polly files sync -s \"polly://data/\" -d \"data/\" -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving ontological names...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/site-packages/polly_validator/downloader/get_valid_names.py:46: UnicodeWarning: unsound encoding, assuming utf-8 (99% confidence)\n",
            "  diseases = pt.Ontology(f'https://raw.githubusercontent.com/bioinfo-el/ontologies/'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting predefined values for Data Type and Data Source...\n",
            "Retrieved Valid Data Types\n",
            "Retrieved Valid Data Sources\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Metadata Files for Validation: 100%|██████████| 4/4 [00:00<00:00, 6136.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒════╤════════════════════════════════════╤══════════════════╤═════════════════════════════════╤═════════════════╤═══════════════╕\n",
            "│    │ dataset_id                         │ Field            │ Original Name                   │ Error Message   │          Repo │\n",
            "╞════╪════════════════════════════════════╪══════════════════╪═════════════════════════════════╪═════════════════╪═══════════════╡\n",
            "│  0 │ ACBC_MSKCC_2015_Copy_Number_AdCC1T │ analysis_summary │ analysis_summary updated thrice │ field required  │ 1654268055800 │\n",
            "╘════╧════════════════════════════════════╧══════════════════╧═════════════════════════════════╧═════════════════╧═══════════════╛\n",
            "╒════╤═════════════════════════════════════╤══════════════════╤═════════════════════════════════╤═════════════════╤═══════════════╕\n",
            "│    │ dataset_id                          │ Field            │ Original Name                   │ Error Message   │          Repo │\n",
            "╞════╪═════════════════════════════════════╪══════════════════╪═════════════════════════════════╪═════════════════╪═══════════════╡\n",
            "│  0 │ ACBC_MSKCC_2015_Copy_Number_AdCC11T │ analysis_summary │ analysis_summary updated thrice │ field required  │ 1654268055800 │\n",
            "╘════╧═════════════════════════════════════╧══════════════════╧═════════════════════════════════╧═════════════════╧═══════════════╛\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Status File of Validation: 100%|██████████| 4/4 [00:00<00:00, 10679.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------err dataset------\n",
            "                            dataset_id             Field  \\\n",
            "0   ACBC_MSKCC_2015_Copy_Number_AdCC1T  analysis_summary   \n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC11T  analysis_summary   \n",
            "\n",
            "                      Original Name   Error Message           Repo  \n",
            "0  analysis_summary updated thrice   field required  1654268055800  \n",
            "1  analysis_summary updated thrice   field required  1654268055800  \n",
            "-------status dataset-----\n",
            "{'ACBC_MSKCC_2015_Copy_Number_AdCC1T': False, 'ACBC_MSKCC_2015_Copy_Number_AdCC11T': False}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Case 1 -> \n",
        "# 4 dataset level metadata Files\n",
        "# 2 files -> validation = true, In that 2 files -> force_ingest => false, 1 file force_ingest => true\n",
        "\n",
        "# step 1 => validation\n",
        "# step 2 => add dataset \n",
        "# 2 Files will be uploaded\n",
        "# File 1 => validation = False\n",
        "# File 2 => validation = True and force_ingest = True\n",
        "# The other 1 files which has validation = True and force_ingest = false => Not Uploaded\n",
        "\n",
        "\n",
        "# ingestion_test_1\n",
        "repo_id = \"1654268055800\"\n",
        "source_folder_path_data = \"/import/data\"\n",
        "source_folder_metadata = \"/import/metadata\"\n",
        "source_folder_path = {\"data\":source_folder_path_data, \"metadata\":source_folder_metadata}\n",
        "\n",
        "from polly.validation import Validation\n",
        "validation = Validation()\n",
        "# step 1\n",
        "err_dataset, status_dataset = validation.validate_datasets(repo_id, source_folder_path)\n",
        "print(\"------err dataset------\")\n",
        "print(err_dataset)\n",
        "\n",
        "print(\"-------status dataset-----\")\n",
        "print(status_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: These ['ACBC_MSKCC_2015_Copy_Number_AdCC1T.json', 'ACBC_MSKCC_2015_Copy_Number_AdCC11T.json'] file have failed validation. These files will only be ingested if force_ingest key in metadata is set to True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Metadata files: 100%|██████████| 3/3 [00:00<00:00, 5552.92it/s]\n",
            "Uploading data files: 100%|██████████| 3/3 [00:00<00:00,  5.23files/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                                  File Name        Message\n",
            "0                   combined_metadata.json  File Uploaded\n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC10T.gct  File Uploaded\n",
            "2  ACBC_MSKCC_2015_Copy_Number_AdCC11T.gct  File Uploaded\n",
            "3  ACBC_MSKCC_2015_Copy_Number_AdCC12T.gct  File Uploaded\n",
            "\n",
            "\n",
            "-----Files which are Not Validated-------\n",
            "                    Invalid Metadata Files\n",
            "0  ACBC_MSKCC_2015_Copy_Number_AdCC1T.json\n",
            "\n",
            "\n",
            "                       Invalid Data Files\n",
            "0  ACBC_MSKCC_2015_Copy_Number_AdCC1T.gct\n",
            "----res upload df-----\n",
            "                                 File Name        Message\n",
            "0                   combined_metadata.json  File Uploaded\n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC10T.gct  File Uploaded\n",
            "2  ACBC_MSKCC_2015_Copy_Number_AdCC11T.gct  File Uploaded\n",
            "3  ACBC_MSKCC_2015_Copy_Number_AdCC12T.gct  File Uploaded\n"
          ]
        }
      ],
      "source": [
        "# step 2\n",
        "# run add dataset step\n",
        "res_upload_df = omixatlas.add_datasets(repo_id, source_folder_path, \"transcriptomics_110\")\n",
        "print(\"----res upload df-----\")\n",
        "print(res_upload_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Metadata Files for Validation: 100%|██████████| 4/4 [00:00<00:00, 6442.86it/s]\n",
            "Generating Status File of Validation: 100%|██████████| 4/4 [00:00<00:00, 7943.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------err dataset------\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "-------status dataset-----\n",
            "{}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Case 2 \n",
        "# all files validation false\n",
        "# 4 dataset level metadata files\n",
        "# validation False -> In all the files -> All the files uploaded\n",
        "\n",
        "# step 1 validation\n",
        "repo_id = \"1654268055800\"\n",
        "source_folder_path_data = \"/import/data\"\n",
        "source_folder_metadata = \"/import/metadata\"\n",
        "source_folder_path = {\"data\":source_folder_path_data, \"metadata\":source_folder_metadata}\n",
        "\n",
        "# step 1\n",
        "err_dataset, status_dataset = validation.validate_datasets(repo_id, source_folder_path)\n",
        "print(\"------err dataset------\")\n",
        "print(err_dataset)\n",
        "\n",
        "print(\"-------status dataset-----\")\n",
        "print(status_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Metadata files: 100%|██████████| 4/4 [00:00<00:00, 8710.91it/s]\n",
            "Uploading data files: 100%|██████████| 4/4 [00:00<00:00,  5.34files/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                                  File Name        Message\n",
            "0                   combined_metadata.json  File Uploaded\n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC10T.gct  File Uploaded\n",
            "2  ACBC_MSKCC_2015_Copy_Number_AdCC11T.gct  File Uploaded\n",
            "3  ACBC_MSKCC_2015_Copy_Number_AdCC12T.gct  File Uploaded\n",
            "4   ACBC_MSKCC_2015_Copy_Number_AdCC1T.gct  File Uploaded\n",
            "\n",
            "\n",
            "-----Files which are Not Validated-------\n",
            "Empty DataFrame\n",
            "Columns: [Invalid Metadata Files]\n",
            "Index: []\n",
            "\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [Invalid Data Files]\n",
            "Index: []\n",
            "----res upload df-----\n",
            "                                 File Name        Message\n",
            "0                   combined_metadata.json  File Uploaded\n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC10T.gct  File Uploaded\n",
            "2  ACBC_MSKCC_2015_Copy_Number_AdCC11T.gct  File Uploaded\n",
            "3  ACBC_MSKCC_2015_Copy_Number_AdCC12T.gct  File Uploaded\n",
            "4   ACBC_MSKCC_2015_Copy_Number_AdCC1T.gct  File Uploaded\n"
          ]
        }
      ],
      "source": [
        "# step 2 -> add datasets\n",
        "res_upload_df = omixatlas.add_datasets(repo_id, source_folder_path, \"transcriptomics_110\")\n",
        "print(\"----res upload df-----\")\n",
        "print(res_upload_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Metadata files: 100%|██████████| 5/5 [00:00<00:00, 9246.70it/s]\n",
            "Uploading data files: 100%|██████████| 5/5 [00:00<00:00,  6.61files/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                                  File Name        Message\n",
            "0                   combined_metadata.json  File Uploaded\n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC10T.gct  File Uploaded\n",
            "2  ACBC_MSKCC_2015_Copy_Number_AdCC11T.gct  File Uploaded\n",
            "3  ACBC_MSKCC_2015_Copy_Number_AdCC12T.gct  File Uploaded\n",
            "4   ACBC_MSKCC_2015_Copy_Number_AdCC1T.gct  File Uploaded\n",
            "5                          test_file_1.gct  File Uploaded\n",
            "\n",
            "\n",
            "-----Files which are Not Validated-------\n",
            "Empty DataFrame\n",
            "Columns: [Invalid Metadata Files]\n",
            "Index: []\n",
            "\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [Invalid Data Files]\n",
            "Index: []\n",
            "----res upload df-----\n",
            "                                 File Name        Message\n",
            "0                   combined_metadata.json  File Uploaded\n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC10T.gct  File Uploaded\n",
            "2  ACBC_MSKCC_2015_Copy_Number_AdCC11T.gct  File Uploaded\n",
            "3  ACBC_MSKCC_2015_Copy_Number_AdCC12T.gct  File Uploaded\n",
            "4   ACBC_MSKCC_2015_Copy_Number_AdCC1T.gct  File Uploaded\n",
            "5                          test_file_1.gct  File Uploaded\n"
          ]
        }
      ],
      "source": [
        "# Case 3 -> Files having validation `false`\n",
        "# file name -> test_file_1\n",
        "# Not gone through validation step -> File was not present when validate datasets were run\n",
        "# But validation is set to False -> This File will get uploaded\n",
        "\n",
        "res_upload_df = omixatlas.add_datasets(repo_id, source_folder_path, \"transcriptomics_110\")\n",
        "print(\"----res upload df-----\")\n",
        "print(res_upload_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "This test_file_2.json has validate set to True but not validated",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Case 4 -> Files having validation `true`\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# file name -> test_file_2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Not gone through validation step -> File was not present when validate datasets were run\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# But validation is set to true -> validation is not run then even if force_ingest = True -> It will raise error\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# As the system knows that -> `test_file_2` has not gone through validation\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m res_upload_df \u001b[38;5;241m=\u001b[39m \u001b[43momixatlas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_folder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscriptomics_110\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----res upload df-----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(res_upload_df)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/tracking.py:42\u001b[0m, in \u001b[0;36mTrack.track_decorator.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m returned_err\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/tracking.py:18\u001b[0m, in \u001b[0;36mTrack.track_decorator.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m execution_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     execution_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/omixatlas.py:1914\u001b[0m, in \u001b[0;36mOmixAtlas.add_datasets\u001b[0;34m(self, repo_id, source_folder_path, destination_folder_path, priority)\u001b[0m\n\u001b[1;32m   1912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_upload_results_df\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 1914\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/omixatlas.py:1817\u001b[0m, in \u001b[0;36mOmixAtlas.add_datasets\u001b[0;34m(self, repo_id, source_folder_path, destination_folder_path, priority)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;66;03m# parameters check\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_check_for_add_dataset(\n\u001b[1;32m   1815\u001b[0m         repo_id, source_folder_path, destination_folder_path, priority\n\u001b[1;32m   1816\u001b[0m     )\n\u001b[0;32m-> 1817\u001b[0m     validation_dataset_lvl \u001b[38;5;241m=\u001b[39m \u001b[43momix_hlpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_status_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_folder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1818\u001b[0m     (\n\u001b[1;32m   1819\u001b[0m         session_tokens,\n\u001b[1;32m   1820\u001b[0m         bucket_name,\n\u001b[1;32m   1821\u001b[0m         package_name,\n\u001b[1;32m   1822\u001b[0m         metadata_directory,\n\u001b[1;32m   1823\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_session_tokens(repo_id, destination_folder_path)\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;66;03m# folder paths\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/omixatlas_hlpr.py:813\u001b[0m, in \u001b[0;36mcheck_status_file\u001b[0;34m(source_folder_path)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m decrypted_status_data_dict\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/omixatlas_hlpr.py:808\u001b[0m, in \u001b[0;36mcheck_status_file\u001b[0;34m(source_folder_path)\u001b[0m\n\u001b[1;32m    804\u001b[0m         decrypted_status_data_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(decrypted_status_data)\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# check if all the files which have `validate:True` is validated or not\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# i.e. their entry is present in status dict or not\u001b[39;00m\n\u001b[0;32m--> 808\u001b[0m         \u001b[43mcheck_all_files_validated\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata_file_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecrypted_status_data_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_folder_path\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m decrypted_status_data_dict\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/omixatlas_hlpr.py:709\u001b[0m, in \u001b[0;36mcheck_all_files_validated\u001b[0;34m(metadata_file_list, decrypted_status_data_dict, metadata_folder_path)\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset Id not present in metadata file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate_param \u001b[38;5;129;01mand\u001b[39;00m dataset_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m decrypted_status_data_dict:\n\u001b[0;32m--> 709\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has validate set to True but not validated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m     )\n",
            "\u001b[0;31mException\u001b[0m: This test_file_2.json has validate set to True but not validated"
          ]
        }
      ],
      "source": [
        "# Case 4 -> Files having validation `true`\n",
        "# file name -> test_file_2\n",
        "# Not gone through validation step -> File was not present when validate datasets were run\n",
        "# But validation is set to true -> validation is not run then even if force_ingest = True -> It will raise error\n",
        "# As the system knows that -> `test_file_2` has not gone through validation\n",
        "\n",
        "res_upload_df = omixatlas.add_datasets(repo_id, source_folder_path, \"transcriptomics_110\")\n",
        "print(\"----res upload df-----\")\n",
        "print(res_upload_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# update dataset cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Metadata Files for Validation: 100%|██████████| 4/4 [00:00<00:00, 5419.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒════╤═════════════════════════════════════╤══════════════════╤═════════════════════════════════╤═════════════════╤═══════════════╕\n",
            "│    │ dataset_id                          │ Field            │ Original Name                   │ Error Message   │          Repo │\n",
            "╞════╪═════════════════════════════════════╪══════════════════╪═════════════════════════════════╪═════════════════╪═══════════════╡\n",
            "│  0 │ ACBC_MSKCC_2015_Copy_Number_AdCC12T │ analysis_summary │ analysis_summary updated thrice │ field required  │ 1654268055800 │\n",
            "╘════╧═════════════════════════════════════╧══════════════════╧═════════════════════════════════╧═════════════════╧═══════════════╛\n",
            "╒════╤═════════════════════════════════════╤══════════════════╤═════════════════════════════════╤═════════════════╤═══════════════╕\n",
            "│    │ dataset_id                          │ Field            │ Original Name                   │ Error Message   │          Repo │\n",
            "╞════╪═════════════════════════════════════╪══════════════════╪═════════════════════════════════╪═════════════════╪═══════════════╡\n",
            "│  0 │ ACBC_MSKCC_2015_Copy_Number_AdCC10T │ analysis_summary │ analysis_summary updated thrice │ field required  │ 1654268055800 │\n",
            "╘════╧═════════════════════════════════════╧══════════════════╧═════════════════════════════════╧═════════════════╧═══════════════╛\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Status File of Validation: 100%|██████████| 4/4 [00:00<00:00, 6445.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------err dataset------\n",
            "                            dataset_id             Field  \\\n",
            "0  ACBC_MSKCC_2015_Copy_Number_AdCC12T  analysis_summary   \n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC10T  analysis_summary   \n",
            "\n",
            "                      Original Name   Error Message           Repo  \n",
            "0  analysis_summary updated thrice   field required  1654268055800  \n",
            "1  analysis_summary updated thrice   field required  1654268055800  \n",
            "-------status dataset-----\n",
            "{'ACBC_MSKCC_2015_Copy_Number_AdCC12T': False, 'ACBC_MSKCC_2015_Copy_Number_AdCC10T': False}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Case 1 -> \n",
        "# 4 dataset level metadata Files\n",
        "# 2 files -> validation = true, In that 2 files -> force_ingest => false, 1 file force_ingest => true\n",
        "\n",
        "# step 1 => validation\n",
        "# step 2 => add dataset \n",
        "# 2 Files will be uploaded\n",
        "# File 1 => validation = False\n",
        "# File 2 => validation = True and force_ingest = True\n",
        "# The other 1 files which has validation = True and force_ingest = false => Not Uploaded\n",
        "\n",
        "\n",
        "repo_id = \"1654268055800\"\n",
        "source_folder_path_data = \"/import/data\"\n",
        "source_folder_metadata = \"/import/metadata\"\n",
        "source_folder_path = {\"data\":source_folder_path_data, \"metadata\":source_folder_metadata}\n",
        "\n",
        "# step 1\n",
        "err_dataset, status_dataset = validation.validate_datasets(repo_id, source_folder_path)\n",
        "print(\"------err dataset------\")\n",
        "print(err_dataset)\n",
        "\n",
        "print(\"-------status dataset-----\")\n",
        "print(status_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: These ['ACBC_MSKCC_2015_Copy_Number_AdCC12T.json', 'ACBC_MSKCC_2015_Copy_Number_AdCC10T.json'] file have failed validation. These files will only be ingested if force_ingest key in metadata is set to True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Metadata files: 100%|██████████| 3/3 [00:00<00:00, 5035.18it/s]\n",
            "Uploading data files: 100%|██████████| 3/3 [00:00<00:00,  6.01files/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                                  File Name        Message\n",
            "0                   combined_metadata.json  File Uploaded\n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC10T.gct  File Uploaded\n",
            "2  ACBC_MSKCC_2015_Copy_Number_AdCC11T.gct  File Uploaded\n",
            "3   ACBC_MSKCC_2015_Copy_Number_AdCC1T.gct  File Uploaded\n",
            "\n",
            "\n",
            "-----Files which are Not Validated-------\n",
            "                     Invalid Metadata Files\n",
            "0  ACBC_MSKCC_2015_Copy_Number_AdCC12T.json\n",
            "\n",
            "\n",
            "                        Invalid Data Files\n",
            "0  ACBC_MSKCC_2015_Copy_Number_AdCC12T.gct\n",
            "----res upload df-----\n",
            "                                 File Name        Message\n",
            "0                   combined_metadata.json  File Uploaded\n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC10T.gct  File Uploaded\n",
            "2  ACBC_MSKCC_2015_Copy_Number_AdCC11T.gct  File Uploaded\n",
            "3   ACBC_MSKCC_2015_Copy_Number_AdCC1T.gct  File Uploaded\n"
          ]
        }
      ],
      "source": [
        "# step 2\n",
        "# run update dataset step - expecting that this should run the same way as add dataset\n",
        "res_upload_df = omixatlas.update_datasets(repo_id, source_folder_path, \"transcriptomics_110\")\n",
        "print(\"----res upload df-----\")\n",
        "print(res_upload_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: These ['ACBC_MSKCC_2015_Copy_Number_AdCC12T.json', 'ACBC_MSKCC_2015_Copy_Number_AdCC10T.json'] file have failed validation. These files will only be ingested if force_ingest key in metadata is set to True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Metadata files: 100%|██████████| 3/3 [00:03<00:00,  1.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                 File Name        Message\n",
            "0  combined_metadata.json  File Uploaded\n",
            "\n",
            "\n",
            "-----Files which are Not Validated-------\n",
            "                     Invalid Metadata Files\n",
            "0  ACBC_MSKCC_2015_Copy_Number_AdCC12T.json\n",
            "\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [Invalid Data Files]\n",
            "Index: []\n",
            "----res upload df-----\n",
            "                File Name        Message\n",
            "0  combined_metadata.json  File Uploaded\n"
          ]
        }
      ],
      "source": [
        "#Case 1.2 same as case1 but only providing the metadata folder path \n",
        "source_folder_path = {\"metadata\":source_folder_metadata}\n",
        "res_upload_df = omixatlas.update_datasets(repo_id, source_folder_path, \"transcriptomics_110\")\n",
        "print(\"----res upload df-----\")\n",
        "print(res_upload_df)\n",
        "#expected: same as above with the only difference being that only the combined metadata file will be uploaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Uploading data files: 100%|██████████| 4/4 [00:00<00:00,  4.93files/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                                  File Name        Message\n",
            "0  ACBC_MSKCC_2015_Copy_Number_AdCC10T.gct  File Uploaded\n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC11T.gct  File Uploaded\n",
            "2  ACBC_MSKCC_2015_Copy_Number_AdCC12T.gct  File Uploaded\n",
            "3   ACBC_MSKCC_2015_Copy_Number_AdCC1T.gct  File Uploaded\n",
            "\n",
            "\n",
            "-----Files which are Not Validated-------\n",
            "Empty DataFrame\n",
            "Columns: [Invalid Metadata Files]\n",
            "Index: []\n",
            "\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [Invalid Data Files]\n",
            "Index: []\n",
            "----res upload df-----\n",
            "                                 File Name        Message\n",
            "0  ACBC_MSKCC_2015_Copy_Number_AdCC10T.gct  File Uploaded\n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC11T.gct  File Uploaded\n",
            "2  ACBC_MSKCC_2015_Copy_Number_AdCC12T.gct  File Uploaded\n",
            "3   ACBC_MSKCC_2015_Copy_Number_AdCC1T.gct  File Uploaded\n"
          ]
        }
      ],
      "source": [
        "#Case 1.3 same as case1 but only providing the data folder path \n",
        "source_folder_path = {\"data\":source_folder_path_data}\n",
        "res_upload_df = omixatlas.update_datasets(repo_id, source_folder_path, \"transcriptomics_110\")\n",
        "print(\"----res upload df-----\")\n",
        "print(res_upload_df)\n",
        "#expected: data files should be updated -> as no metadata file is there. All metadata files be uploaded, as its metadata \n",
        "# have been uploaded previously during add dataset operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#case 2: updating metadata file which has not been uploaded. \n",
        "\n",
        "polly@jupyter-1667926485791899-2dcafce94edd524d56bf23f86ab03e6579-2d0:/import/updateDataset$ ls *\n",
        "data:\n",
        "ACBC_MSKCC_2015_Copy_Number_AdCC10T.gct\n",
        "\n",
        "metadata:\n",
        "\n",
        "ACBC_MSKCC_2015_Copy_Number_AdCC10T.json\n",
        "{\n",
        "    \"__index__\": {\n",
        "        \"file_metadata\": \"true\",\n",
        "        \"col_metadata\": \"true\",\n",
        "        \"row_metadata\": \"true\",\n",
        "        \"data_required\": \"true\",\n",
        "        \"validation_check\": {\n",
        "            \"dataset\": {\n",
        "                \"validate\": false,\n",
        "                \"scope\": \"advanced\",\n",
        "                \"force_ingest\": true\n",
        "            },\n",
        "            \"sample\": {\n",
        "                \"validate\": \"true\",\n",
        "                \"scope\": \"advanced\",\n",
        "                \"force_ingest\": \"false\"\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"curated_file_type\": null,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "Metadata File ACBC_MSKCC_2015_Copy_Number_AdCC10T.json have not been validated and `validate` is set to True. Please run the validation step first.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m source_folder_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/import/updateDataset/metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m source_folder_path \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m:source_folder_metadata}\n\u001b[0;32m----> 7\u001b[0m res_upload_df \u001b[38;5;241m=\u001b[39m \u001b[43momixatlas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_folder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscriptomics_110\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----res upload df-----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(res_upload_df)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/tracking.py:42\u001b[0m, in \u001b[0;36mTrack.track_decorator.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m returned_err\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/tracking.py:18\u001b[0m, in \u001b[0;36mTrack.track_decorator.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m execution_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     execution_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/omixatlas.py:2001\u001b[0m, in \u001b[0;36mOmixAtlas.update_datasets\u001b[0;34m(self, repo_id, source_folder_path, destination_folder_path, priority)\u001b[0m\n\u001b[1;32m   1989\u001b[0m     file_status_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_metadata_data(\n\u001b[1;32m   1990\u001b[0m         repo_id,\n\u001b[1;32m   1991\u001b[0m         source_folder_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1998\u001b[0m         validation_dataset_lvl,\n\u001b[1;32m   1999\u001b[0m     )\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 2001\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_status_dict:\n\u001b[1;32m   2004\u001b[0m     result_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generating_response_from_status_dict(\n\u001b[1;32m   2005\u001b[0m         file_status_dict, result_list\n\u001b[1;32m   2006\u001b[0m     )\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/omixatlas.py:1952\u001b[0m, in \u001b[0;36mOmixAtlas.update_datasets\u001b[0;34m(self, repo_id, source_folder_path, destination_folder_path, priority)\u001b[0m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_check_for_update_dataset(\n\u001b[1;32m   1948\u001b[0m     repo_id, source_folder_path, destination_folder_path, priority\n\u001b[1;32m   1949\u001b[0m )\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_folder_path\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1952\u001b[0m     validation_dataset_lvl \u001b[38;5;241m=\u001b[39m \u001b[43momix_hlpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_status_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_folder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1954\u001b[0m     validation_dataset_lvl \u001b[38;5;241m=\u001b[39m {}\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/omixatlas_hlpr.py:813\u001b[0m, in \u001b[0;36mcheck_status_file\u001b[0;34m(source_folder_path)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m decrypted_status_data_dict\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/omixatlas_hlpr.py:796\u001b[0m, in \u001b[0;36mcheck_status_file\u001b[0;34m(source_folder_path)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# if status file not present\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m encrypted_status_data:\n\u001b[0;32m--> 796\u001b[0m     empty_status_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_validation_false_in_all_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_file_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_folder_path\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m empty_status_dict\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m encrypted_status_data:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;66;03m# decrypt status file\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polly/omixatlas_hlpr.py:744\u001b[0m, in \u001b[0;36mcheck_validation_false_in_all_files\u001b[0;34m(metadata_file_list, metadata_folder_path)\u001b[0m\n\u001b[1;32m    733\u001b[0m         validate_param \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    734\u001b[0m             res_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__index__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    735\u001b[0m             \u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_check\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    736\u001b[0m             \u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    737\u001b[0m             \u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    738\u001b[0m         )\n\u001b[1;32m    740\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m validate_param:\n\u001b[1;32m    741\u001b[0m             \u001b[38;5;66;03m# for this file `validate:True`\u001b[39;00m\n\u001b[1;32m    742\u001b[0m             \u001b[38;5;66;03m# this means file has not gone through validation\u001b[39;00m\n\u001b[1;32m    743\u001b[0m             \u001b[38;5;66;03m# raise error\u001b[39;00m\n\u001b[0;32m--> 744\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ValidationError(\n\u001b[1;32m    745\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata File \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m have not been validated and `validate` is set to True. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    746\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run the validation step first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    747\u001b[0m             )\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# return empty dictionary\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;66;03m# no status data present\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;66;03m# no error is raised\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
            "\u001b[0;31mValidationError\u001b[0m: Metadata File ACBC_MSKCC_2015_Copy_Number_AdCC10T.json have not been validated and `validate` is set to True. Please run the validation step first."
          ]
        }
      ],
      "source": [
        "# case 3: update datasets without any prior validation\n",
        "# mve data and metadata files to a different location, where validation_status.txt file not there\n",
        "repo_id = \"1654268055800\"\n",
        "source_folder_path_data = \"/import/updateDataset/data\"\n",
        "source_folder_metadata = \"/import/updateDataset/metadata\"\n",
        "source_folder_path = {\"metadata\":source_folder_metadata}\n",
        "res_upload_df = omixatlas.update_datasets(repo_id, source_folder_path, \"transcriptomics_110\")\n",
        "print(\"----res upload df-----\")\n",
        "print(res_upload_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Metadata Files for Validation: 100%|██████████| 4/4 [00:00<00:00, 6831.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒════╤═════════════════════════════════════╤══════════════════╤═════════════════════════════════╤═════════════════╤═══════════════╕\n",
            "│    │ dataset_id                          │ Field            │ Original Name                   │ Error Message   │          Repo │\n",
            "╞════╪═════════════════════════════════════╪══════════════════╪═════════════════════════════════╪═════════════════╪═══════════════╡\n",
            "│  0 │ ACBC_MSKCC_2015_Copy_Number_AdCC10T │ analysis_summary │ analysis_summary updated thrice │ field required  │ 1654268055800 │\n",
            "╘════╧═════════════════════════════════════╧══════════════════╧═════════════════════════════════╧═════════════════╧═══════════════╛\n",
            "╒════╤═════════════════════════════════════╤══════════════════╤═════════════════════════════════╤═════════════════╤═══════════════╕\n",
            "│    │ dataset_id                          │ Field            │ Original Name                   │ Error Message   │          Repo │\n",
            "╞════╪═════════════════════════════════════╪══════════════════╪═════════════════════════════════╪═════════════════╪═══════════════╡\n",
            "│  0 │ ACBC_MSKCC_2015_Copy_Number_AdCC12T │ analysis_summary │ analysis_summary updated thrice │ field required  │ 1654268055800 │\n",
            "╘════╧═════════════════════════════════════╧══════════════════╧═════════════════════════════════╧═════════════════╧═══════════════╛\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Status File of Validation: 100%|██████████| 4/4 [00:00<00:00, 11444.21it/s]\n"
          ]
        }
      ],
      "source": [
        "#case 3.1 - update just metadta files after the validation has been done\n",
        "source_folder_path = {\"metadata\":source_folder_metadata, \"data\": source_folder_path_data}\n",
        "err_dataset, status_dataset = validation.validate_datasets(repo_id, source_folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: These ['ACBC_MSKCC_2015_Copy_Number_AdCC10T.json', 'ACBC_MSKCC_2015_Copy_Number_AdCC12T.json'] file have failed validation. These files will only be ingested if force_ingest key in metadata is set to True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Metadata files: 100%|██████████| 3/3 [00:02<00:00,  1.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                 File Name        Message\n",
            "0  combined_metadata.json  File Uploaded\n",
            "\n",
            "\n",
            "-----Files which are Not Validated-------\n",
            "                     Invalid Metadata Files\n",
            "0  ACBC_MSKCC_2015_Copy_Number_AdCC12T.json\n",
            "\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [Invalid Data Files]\n",
            "Index: []\n",
            "----res upload df-----\n",
            "                File Name        Message\n",
            "0  combined_metadata.json  File Uploaded\n"
          ]
        }
      ],
      "source": [
        "##NOTE: CURRENTLY FOR UPDATE THE VALIDATION OUTPUT FILE IS REQUIRED IN THE METADATA FOLDER\n",
        "source_folder_path = {\"metadata\":source_folder_metadata}\n",
        "res_upload_df = omixatlas.update_datasets(repo_id, source_folder_path, \"transcriptomics_110\")\n",
        "print(\"----res upload df-----\")\n",
        "print(res_upload_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# case 4 : trying to update a metadta file for which the corresponding data file is not present in the OA. \n",
        "#on terminal we did: \n",
        "#/import/updateDataset/metadata$ rm validation_status.txt\n",
        "#/import/updateDataset/metadata$ mv ACBC_MSKCC_2015_Copy_Number_AdCC10T.json ACBC_MSKCC_2015_Copy_Number_AdCC10T_2.json\n",
        "#/import/updateDataset/data$ mv ACBC_MSKCC_2015_Copy_Number_AdCC10T.gct ACBC_MSKCC_2015_Copy_Number_AdCC10T_2.gct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Metadata Files for Validation: 100%|██████████| 4/4 [00:00<00:00, 7636.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒════╤═════════════════════════════════════╤══════════════════╤═════════════════════════════════╤═════════════════╤═══════════════╕\n",
            "│    │ dataset_id                          │ Field            │ Original Name                   │ Error Message   │          Repo │\n",
            "╞════╪═════════════════════════════════════╪══════════════════╪═════════════════════════════════╪═════════════════╪═══════════════╡\n",
            "│  0 │ ACBC_MSKCC_2015_Copy_Number_AdCC12T │ analysis_summary │ analysis_summary updated thrice │ field required  │ 1654268055800 │\n",
            "╘════╧═════════════════════════════════════╧══════════════════╧═════════════════════════════════╧═════════════════╧═══════════════╛\n",
            "╒════╤═════════════════════════════════════╤══════════════════╤═════════════════════════════════╤═════════════════╤═══════════════╕\n",
            "│    │ dataset_id                          │ Field            │ Original Name                   │ Error Message   │          Repo │\n",
            "╞════╪═════════════════════════════════════╪══════════════════╪═════════════════════════════════╪═════════════════╪═══════════════╡\n",
            "│  0 │ ACBC_MSKCC_2015_Copy_Number_AdCC10T │ analysis_summary │ analysis_summary updated thrice │ field required  │ 1654268055800 │\n",
            "╘════╧═════════════════════════════════════╧══════════════════╧═════════════════════════════════╧═════════════════╧═══════════════╛\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Status File of Validation: 100%|██████████| 4/4 [00:00<00:00, 5601.74it/s]\n"
          ]
        }
      ],
      "source": [
        "# rerun validation \n",
        "source_folder_path = {\"metadata\":source_folder_metadata, \"data\": source_folder_path_data}\n",
        "err_dataset, status_dataset = validation.validate_datasets(repo_id, source_folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: These ['ACBC_MSKCC_2015_Copy_Number_AdCC12T.json', 'ACBC_MSKCC_2015_Copy_Number_AdCC10T_3.json'] file have failed validation. These files will only be ingested if force_ingest key in metadata is set to True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Metadata files:  67%|██████▋   | 2/3 [00:02<00:01,  1.01s/it]WARNING: Unable to update metadata/data file ACBC_MSKCC_2015_Copy_Number_AdCC10T_3.json because corresponding data/metadata file not present in OmixAtlas. Please add the files using add_datasets function. For any questions, please reach out to polly.support@elucidata.io. \n",
            "Processing Metadata files: 100%|██████████| 3/3 [00:02<00:00,  1.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                 File Name        Message\n",
            "0  combined_metadata.json  File Uploaded\n",
            "\n",
            "\n",
            "-----Files which are Not Validated-------\n",
            "                     Invalid Metadata Files\n",
            "0  ACBC_MSKCC_2015_Copy_Number_AdCC12T.json\n",
            "\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [Invalid Data Files]\n",
            "Index: []\n",
            "----res upload df-----\n",
            "                File Name        Message\n",
            "0  combined_metadata.json  File Uploaded\n"
          ]
        }
      ],
      "source": [
        "#updating just the metadata file of which the data file has not been uploaded before \n",
        "source_folder_path = {\"metadata\":source_folder_metadata}\n",
        "res_upload_df = omixatlas.update_datasets(repo_id, source_folder_path, \"transcriptomics_110\")\n",
        "print(\"----res upload df-----\")\n",
        "print(res_upload_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Unable to update metadata/data file ACBC_MSKCC_2015_Copy_Number_AdCC10T_3.gct because corresponding data/metadata file not present in OmixAtlas. Please add the files using add_datasets function. For any questions, please reach out to polly.support@elucidata.io. \n",
            "Uploading data files: 100%|██████████| 3/3 [00:00<00:00,  4.93files/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                                  File Name        Message\n",
            "0  ACBC_MSKCC_2015_Copy_Number_AdCC11T.gct  File Uploaded\n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC12T.gct  File Uploaded\n",
            "2   ACBC_MSKCC_2015_Copy_Number_AdCC1T.gct  File Uploaded\n",
            "\n",
            "\n",
            "-----Files which are Not Validated-------\n",
            "Empty DataFrame\n",
            "Columns: [Invalid Metadata Files]\n",
            "Index: []\n",
            "\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [Invalid Data Files]\n",
            "Index: []\n",
            "----res upload df-----\n",
            "                                 File Name        Message\n",
            "0  ACBC_MSKCC_2015_Copy_Number_AdCC11T.gct  File Uploaded\n",
            "1  ACBC_MSKCC_2015_Copy_Number_AdCC12T.gct  File Uploaded\n",
            "2   ACBC_MSKCC_2015_Copy_Number_AdCC1T.gct  File Uploaded\n"
          ]
        }
      ],
      "source": [
        "#updating just the data file of which the data file has not been uploaded before -> behaviour as expected\n",
        "source_folder_path = {\"data\": source_folder_path_data}\n",
        "res_upload_df = omixatlas.update_datasets(repo_id, source_folder_path, \"transcriptomics_110\")\n",
        "print(\"----res upload df-----\")\n",
        "print(res_upload_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}